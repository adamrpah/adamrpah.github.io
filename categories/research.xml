<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NP Complete Heart (Posts about research)</title><link>http://www.npcompleteheart.com/</link><description></description><atom:link href="http://www.npcompleteheart.com/categories/research.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Thu, 01 Mar 2018 19:09:46 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Follow me on the Reading Rainbow</title><link>http://www.npcompleteheart.com/posts/follow-me-on-the-reading-rainbow.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div class="row"&gt;
&lt;div class="col-md-4"&gt;
    &lt;img src="http://www.npcompleteheart.com/images/everything_is_obvious.png"&gt;
&lt;/div&gt;
&lt;div class="col-md-8"&gt;
Over the summer I've been able to successfully chisel away at my goal of reading 20 books by 16
different authors in 2015, with my two weeks of travel to and around Spain providing me ample time
to dive into my iPad and digest. For this tour, I went back in time and read Nicholas Christakis'
&lt;strong&gt;Connected&lt;/strong&gt;, which actually provided me with some insight on researchers and topics that I thought I
already knew pretty well. In terms of an overview of how networks affect social phenomena, I would
highly recommend it. I also plowed through Nate Silver's &lt;strong&gt;The Signal and The Noise&lt;/strong&gt;, because I'm
convinced that being acquainted with Nate Silver's work is the litmus test in the public for
understanding data science. It was pretty good, although some of his metaphors were clunky and only
really stuck with me because of reading fivethirtyeight (his own data science news blog) since its
inception.
&lt;br&gt; &lt;br&gt;
&lt;/div&gt;

&lt;!-- TEASER_END --&gt;
&lt;div class="col-md-12"&gt;
However, it was the third book that really stuck with me. Duncan Watts' "Everything is Obvious
...once you know the answer" is a quick tour through his work starting with Steven Strogatz at
Cornell and covering his work at Columbia University and Yahoo Research. The opening vignette, of
how his roommate reads a newspaper article about something silly in sociological research and
remarks to him that "He should switch to the social sciences and solve all the problems in a week".
That remark, which has been said many times by physicists (both positively and negatively as they
continue to enter new fields) sets us out on our journey.
&lt;br&gt; &lt;br&gt;

The reality is that it's been a decade, and while Duncan and many others quantitative scientists
have published great work investigating social science problems, understanding of social systems and
human behavior remains as elusive today as 20 years ago. If anything, what we've learned is that
randomness and unseen or unmeasurable effects have the ability to impede making predictions that go
far into the future.
&lt;br&gt; &lt;br&gt;

A great experiment from Watts’ team demonstrates this: users were able to go to a website (called
Music Lab) and listen to music for free. After listening to a song they would be asked to rate it
(on a 5 point scale) and were then given the opportunity to download it. However, there were
multiple 'worlds' constructed for the users (all containing the same songs). In the first world, the
songs were merely displayed, in the other worlds the songs were ranked by and displayed the number
of downloads. 
&lt;br&gt; &lt;br&gt;

Now, if behavior was in any way predictable, the end ratings and downloads from these different
worlds should match. With large enough populations the true 'quality' of a song should shine through
and there should be a strong correlation between ratings in the different “worlds”.  
&lt;br&gt; &lt;br&gt;

Not at all. When we allow for social influence, the winners become even bigger winners. Furthermore,
the 'more' that they win is highly variable between worlds. A song that gets a high rating early
accumulates more high ratings. And the initial ratings are extremely stochastic. 
&lt;br&gt; &lt;br&gt;

This may seem obvious in hindsight (the name of the book, right?), but it's really not. Behaviors
are erratic, and difficult to predict. If the social world behaved like the physical world,
individuals would have a negligible influence on each other. Or at least, in larger populations the
randomness would become less dominant. Of course, we don’t see this. The social world is weird,
chaotic, and difficult to predict. Not exactly the unifying laws that came from Physics after all. 
&lt;/div&gt;
&lt;/div&gt;</description><category>book</category><category>research</category><category>review</category><category>social influence</category><category>social science</category><category>socialdna</category><category>socialdna readings</category><guid>http://www.npcompleteheart.com/posts/follow-me-on-the-reading-rainbow.html</guid><pubDate>Sun, 16 Aug 2015 14:41:17 GMT</pubDate></item><item><title>Computational research, no longer a red-headed stepchild!</title><link>http://www.npcompleteheart.com/posts/computational-research-no-longer-a-red-headed-stepchild.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div class="row"&gt;
&lt;div class="col-md-8"&gt;
Research Day as a chairperson and poster judge.I typically cringe at the thought of attending
conferences and symposia, since I am mainly a homebody (I love my desk, computer, research, and
daily schedule), but at the symposium on Tuesday I felt constantly excited and engaged. The variety
and quality of research presented was excellent and the diverse topics covered kept the gears
turning in my head.
&lt;/div&gt;
&lt;div class="col-md-4"&gt;
&lt;img src="http://www.npcompleteheart.com/images/computational_research_day.jpg" height="150px"&gt;
&lt;/div&gt;

&lt;!-- TEASER_END --&gt;

&lt;div class="col-md-12"&gt;
Most importantly, the sense of camaraderie that I felt amongst these computational researchers kept
me energized. I wish that this event existed when I started graduate school. As a computational
researcher that was purportedly a biologist, it was easy to feel that I was a part of a group that I
was a red-headed stepchild. My home program was primarily concerned with experimentation or the
application of rote computational biology (i.e. anything involving a computer that would have been
published since the early 90s). Most of the other departments that included some computational
research were ones where computational methods were de rigeur and field specific research dominated.
Instead I was looking more for what made me gravitate towards the Northwestern Institute for
complexity, computational researchers that were applying new techniques and conducting
interdisciplinary research.
&lt;br&gt;&lt;br&gt;
I can’t wait to see what new problems people will tackle as the quantity, diversity, and
interdisciplinarity of computational research continues to expand. Because of my experience in
biology, I realize how important it is to bring other fields and researcher into the fold of the
computational community. It will help change how people view their research problems and hopefully
lead to better research. This is what drives me to help put on the bootcamps as an introduction to
programming and data science. It’s also why I believe that events like the Computational Research
Day are integral to a healthy, productive research environment at a University.
&lt;br&gt;&lt;br&gt;
Keep Growing, Computational Research! Keep Growing!
&lt;/div&gt;
&lt;/div&gt;</description><category>conference</category><category>research</category><guid>http://www.npcompleteheart.com/posts/computational-research-no-longer-a-red-headed-stepchild.html</guid><pubDate>Wed, 22 Apr 2015 00:56:52 GMT</pubDate></item><item><title>MongoDB is for researchers</title><link>http://www.npcompleteheart.com/posts/mongodb-is-for-researchers.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div class="row"&gt;
&lt;div class="col-md-4"&gt;
&lt;img src="http://www.npcompleteheart.com/images/mongodb.jpeg"&gt;
&lt;/div&gt;
&lt;div class="col-md-8"&gt;
Over the past three years I’ve been something of an evangelist for using MongoDB. This stance has
drawn derision from some outside the lab, which frequently forces me to clarify in what
circumstances I think MongoDB (or NoSQL in general) is so great. Unfortunately, I’ve been too lazy
to put those thoughts into writing, so this is my long overdue explanation and the first in a series
of posts describing how I use MongoDB daily.
&lt;/div&gt;

&lt;!-- TEASER_END --&gt;

&lt;div class="col-md-12"&gt;
&lt;h3&gt;MongoDB is for &lt;strike&gt;lovers&lt;/strike&gt; researchers and scientists&lt;/h3&gt;

So I think the first question to tackle is why use a database at all? Here are the three basic
reasons that caused me to make a switch.
&lt;br&gt;&lt;br&gt;

&lt;li&gt;&lt;strong&gt;Speed.&lt;/strong&gt; If you’ve ever explored the parameter landscape of a model then you’ve likely
experienced the point when typing `ls *` in your results folder can bring your system to its knees.
While there are ways to work around this problem (creating subfolders, smart naming conventions to
get groups of files), you can also just switch to storing the results in a database. Databases are
designed for storing millions of records easily.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Queries.&lt;/strong&gt; How many runs with parameters (rho, mu, sigma) have finished? Okay, now how many
of those runs have a final value of y? Not nearly as easy to answer is it? At best it would require
looking at the final line of every file. At worst, with some odd encoding scheme or additional forms
of output in the same file this could require parsing every file. Databases make answering these
types of questions quick and easy.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Portability.&lt;/strong&gt; Your data files need to be in a specified directory for your code to read
it. A folder for all your results needs to be in the right place too. Any time you want to test your
code those parts all need to be there and hopefully the ‘there’ isn’t in the same folder as your
code (I am an unabashed proponent of separating code, data, and results). For me, switching between
a laptop, workstation, and two different clusters, this can lead to some annoying inconsistencies
with file availability (specifically with large data files). Storing your data in a database with a
static IP address makes it easy to access anywhere.&lt;/li&gt;
&lt;br&gt;&lt;br&gt;

This is the start to why I think it’s a good idea to use a database, with another lurking reason
being that working knowledge of databases is required outside academia. If none of those reasons
resonate with you to explore these options then don’t worry, not everyone has the same research
problems as me.
&lt;br&gt;&lt;br&gt;

&lt;h3&gt;So why X technology over Y technology?&lt;/h3&gt;

If you wade too far into the internet you’ll find out that MongoDB is a type of NoSQL database and
that other types of databases are SQL databases. If you wade even a little bit further then a
torrent of flame wars will come pouring out of your monitor and you should just shut your eyes,
cover your ears, and pull your computer’s power plug out. Hopefully the following reasons will make
it slightly clear what the differences are without having to go dive into the recesses of the
internet. 
&lt;br&gt;&lt;br&gt;

Ease of use - i.e. Schemaless. What does schemaless really mean?
When you use a SQL database you need to first create a database and then a table. Then once you
create a table you must give commands or use a GUI to establish the number of columns, the names of
columns, and, most importantly, the data type that each column can hold. When a record is entered
into the database it must have all those fields. If you decide to change your code and need to store
additional data fields then you must alter the table first (or else suffer an error!).
&lt;br&gt;&lt;br&gt;

With MongoDB you create a database and then insert a document into it. It will even lazily create
the collection that you told it to use. It can have any number of fields (or keys in Mongo/document
speak) and each key can be named however you want. The twist is when you go to insert a second
document. The second document doesn’t need the same number of keys, or key names, or even the
datatypes of the values associated with each key name. It allows you to do whatever at any time,
with any document in the database.
&lt;br&gt;&lt;br&gt;

Now this freedom is considered to be a flaw in some minds, but all I see is that the onus of
consistency is on the programmer (i.e. you). In the context of a single person, a small group, or a
research lab I don’t think that it’s much to expect that everyone act responsibly and document what
they’re doing (either in the README for the project and/or with explicit key names). The most
important thing to remember is that just about any technology can be detrimental to the workings of
a project if in the hands of an irresponsible idiot.
&lt;br&gt;&lt;br&gt;

&lt;h4&gt;Dictionaries!&lt;/h4&gt;

So this stems from being a pythonista, but when  I code I store things as dictionaries or classes
typically. MongoDB lets me shove that directly into the database since it works with natively with
dictionaries (Mongo’s data store is a BSON, which is more or less a JSON, which is almost a
dictionary). This isn’t so when working with a SQL database, since each record is stored in a row
(think of a CSV file), and for me this is a huge differentiation and selling point. 
&lt;br&gt;&lt;br&gt;

Complex data structures can be natively stored in MongoDB and they are directly returned when I
query them.  So for me, when I run a simulation and there is a class keeping track of the time
evolution of the system at the end of the run I can just calculate whatever additional metrics are
necessary and save the dictionary into the database. When I need to analyze the results I can either
roll directly with the dictionary and start analyzing, it’s pretty simple.
&lt;br&gt;&lt;br&gt;

&lt;h4&gt;Complex values.&lt;/h4&gt;
I have stored datetime as a value, which isn’t really that special. What is special is when I store
a networkx graph object. Mongo will let you shove a fair number of things into it without requiring
you to convert them to a string. This is not only handy, but it cuts out code and processing steps
on file loading.
&lt;br&gt;&lt;br&gt;

&lt;h4&gt;MapReduce.&lt;/h4&gt;
This is more of a footnote but MapReduce is a great feature and can turn 24 hours of computation
time into one fairly quickly. 
&lt;br&gt;&lt;br&gt;

These are the basic reasons why I use MongoDB, both in comparison to a file system and a SQL
database. I will never say that it’s the fastest or the best solution from a technical standpoint,
but it is the quickest and easiest solution in regards to my time, which is the most important thing
in my mind. I’m a researcher, not someone setting up production databases or something soul crushing
like that ;)
&lt;/div&gt;
&lt;/div&gt;</description><category>mongodb</category><category>research</category><guid>http://www.npcompleteheart.com/posts/mongodb-is-for-researchers.html</guid><pubDate>Tue, 31 Mar 2015 18:49:20 GMT</pubDate></item><item><title>Let me just scale this figure down 3 times, then skew it 1.5 times</title><link>http://www.npcompleteheart.com/posts/let-me-just-scale-this-figure-down-3-times-then-skew-it-15-times.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img alt="journal image" src="http://www.npcompleteheart.com/images/publishing_figures.png"&gt;&lt;/p&gt;
&lt;p&gt;
I won't lie, I don't find the process of submitting an academic manuscript or even the post-acceptance rigmarole to be pleasurable. However, the part of this process that most makes me want to put my head through the wall is the insistence of most journals on content being submitted in word documents and raster images.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;This insistence is asinine.&lt;/b&gt;
&lt;/p&gt;

&lt;!-- TEASER_END --&gt;

&lt;p&gt;
Now let me be clear, I'm not stupid. I know that supporting additional input formats could make the process marginally more difficult or expensive for the journals to accommodate. But honestly, how much more could it reasonably cost to support pdf and vector images in the editorial process?
&lt;/p&gt;

&lt;h4&gt;blah blah blah, why does this even matter?&lt;/h4&gt;

&lt;p&gt;
I &lt;b&gt;care&lt;/b&gt; about what my work looks like. While I am more than satisfied with the formatting on manuscript texts that academic journals provide, I am far less happy with the support for images and the typical insistence on rasterized images being provided. Why is this such a sticking point for me? Because raster images look like ass when they are altered from their original size (some more than others, but all to some degree.
&lt;/p&gt;

&lt;p&gt;
The size that any arbitrary journal decides to publish your figure in is relatively arbitrary as far as I can tell, it may or may not be compressed or stretched and margins will differ between different journal templates. When I provide a vector image (in eps/ps/svg format for those interested) the graphic can be scaled to within reason and still look nice and crisp as I intended in a relatively small file size. While it won't be perfect (text may get too small or big if it's scaled down) at the very least it will always look at least &lt;b&gt;good&lt;/b&gt; (in my mind at least) and, more importantly, &lt;b&gt;crisp&lt;/b&gt;.
&lt;/p&gt;

&lt;p&gt;
Raster images (png/jpg/tiff for the curious) provide none of that. Raster images are built for the size of the original canvas since they are a bitmap. To change the size in any way means either discarding pixel information or inferring it based on the color values of nearby, known cells in the pixel grid. This means that changing the size causes a loss in crispness, &lt;b&gt;especially for text&lt;/b&gt;, and that your resulting image will end up looking like ass.
&lt;/p&gt;

&lt;p&gt; 
I say this because in my last paper I provided an eps image of the chart that I've set as the image of this post. It looks fine at most sizes as an eps, until it gets rather small and that's largely because it needs to be a rather large chart. But when the tiff file for it is resized, it ends up looking rather horrible. However, since this is such a large chart I need to know the dimensions that it will be published as in order for me to optimize the raster image, which I can't do until the proof stage. So now the process becomes:
&lt;br&gt;
&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;Submit placeholder image to journal&lt;/li&gt;
&lt;li&gt;Go through review process&lt;/li&gt;
&lt;li&gt;Get accepted proof, measure published image dimensions&lt;/li&gt;
&lt;li&gt;Resubmit altered raster image to published dimensions&lt;/li&gt;
&lt;/ol&gt;
&lt;br&gt;
All told, I think that process &lt;b&gt;sucks&lt;/b&gt; for both me and the journal. Especially because the method for distribution is predominantly moving to the internet where &lt;b&gt;none of these concerns even matter&lt;/b&gt;.
&lt;/div&gt;</description><category>academia</category><category>hair pull</category><category>research</category><category>soapbox</category><guid>http://www.npcompleteheart.com/posts/let-me-just-scale-this-figure-down-3-times-then-skew-it-15-times.html</guid><pubDate>Sun, 16 Nov 2014 14:39:55 GMT</pubDate></item><item><title>Ever wanted to estimate small area effects in health data?</title><link>http://www.npcompleteheart.com/posts/ever-wanted-to-estimate-small-area-effects-in-health-data.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;h5&gt;
Well now you sort of can!
&lt;/h5&gt;

&lt;p&gt;
I just presented at AMIA on my methodology to use limited patient demographic/location data to estimate the number of patient cases at a smaller geographic locality. The methodology is split between a Monte Carlo simulation and GIS methods (Semi-variogram+Kriged Surface+Geographical Gaussian Simulation) to estimate patient cases. Currently the monte carlo part is already coded and freely available on my &lt;strike&gt;bitbucket&lt;/strike&gt;. The latter half of the code is coming as soon as we can identify an appropriate open source GIS library to port it the current code into.
&lt;/p&gt;

&lt;!-- TEASER_END --&gt;

&lt;iframe src="http://www.slideshare.net/slideshow/embed_code/33430482" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen&gt; &lt;/iframe&gt;

&lt;p&gt;&lt;/p&gt;&lt;div style="margin-bottom:5px"&gt; &lt;strong&gt; &lt;a href="https://www.slideshare.net/AdamPah/amia-cri-2014-san-francisco-patient-privacy-small-area-estimation-using-monte-carlo-methods" title="AMIA CRI 2014 San Francisco Patient Privacy Small Area Estimation using Monte Carlo Methods" target="_blank"&gt;AMIA CRI 2014 San Francisco Patient Privacy Small Area Estimation using Monte Carlo Methods&lt;/a&gt; &lt;/strong&gt; from &lt;strong&gt;&lt;a href="http://www.slideshare.net/AdamPah" target="_blank"&gt;Adam Pah&lt;/a&gt;&lt;/strong&gt; &lt;/div&gt;&lt;/div&gt;</description><category>presentation</category><category>research</category><guid>http://www.npcompleteheart.com/posts/ever-wanted-to-estimate-small-area-effects-in-health-data.html</guid><pubDate>Fri, 11 Apr 2014 13:00:27 GMT</pubDate></item><item><title>Better late than never</title><link>http://www.npcompleteheart.com/posts/better-late-than-never.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;
I've been meaning to post my slides from my dissertation presentation for awhile. In general, I'm committing myself to practicing more "science in the open" (meaning my presentations/posters/code), so here is the embed link. Better late than never I always say.
&lt;/p&gt;

&lt;p&gt;
I gave this presentation at Northwestern University on June 4, 2013 in order to attain the degree of Doctor of Philosophy in Biological Sciences. It goes along with the publication &lt;a href="http://www.nature.com/srep/2013/130422/srep01695/full/srep01695.html"&gt;"Use of a global metabolic network to curate organismal metabolic networks"&lt;/a&gt;. It also goes along with the &lt;a href="http://www.npcompleteheart.com/posts/metexplore.npcompleteheart.com"&gt;MetExplore&lt;/a&gt; application that I've detailed in the projects tab.
&lt;/p&gt;

&lt;!-- TEASER_END --&gt;

&lt;iframe src="http://www.slideshare.net/slideshow/embed_code/30983226" width="476" height="400" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"&gt;&lt;/iframe&gt;&lt;/div&gt;</description><category>presentation</category><category>research</category><guid>http://www.npcompleteheart.com/posts/better-late-than-never.html</guid><pubDate>Sat, 08 Feb 2014 12:49:13 GMT</pubDate></item><item><title>Holy crap, I published a paper!</title><link>http://www.npcompleteheart.com/posts/holy-crap-i-published-a-paper.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img alt="Publication!" src="http://www.npcompleteheart.com/images/scireports_publication.png"&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href="http://www.nature.com/srep/2013/130422/srep01695/full/srep01695.html"&gt;Scientific
Reports&lt;/a&gt;.  Just glad to have it published finally. I gave a more complete write up of what this
work means on the &lt;a href="http://amarallab.org/blogs/2013/apr/26/framework-understand-cellular-processes/"&gt;Amaral
Lab&lt;/a&gt; website.&lt;/p&gt;&lt;/div&gt;</description><category>research</category><guid>http://www.npcompleteheart.com/posts/holy-crap-i-published-a-paper.html</guid><pubDate>Mon, 22 Apr 2013 12:37:04 GMT</pubDate></item></channel></rss>