<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NP Complete Heart</title><link>http://www.npcompleteheart.com/</link><description>This is a place for the musings, thoughts, and technical notes of Adam Pah.</description><atom:link href="http://www.npcompleteheart.com/rss.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Fri, 01 May 2015 14:47:14 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>I made a Wordpress site on Heroku!</title><link>http://www.npcompleteheart.com/posts/i-made-a-wordpress-site-on-heroku.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;Well, not for me of course. After a solid week of my girlfriend helping me to use Illustrator it was
time to pay her back and help her get a website up and running. I went over the options that I knew
(Nikola, Django, Node.js, Ruby on Rails) and it became quickly apparent that none of those were
options that she liked. Instead she wanted to take a wordpress site that she made in a class and add
some more to it/fix it up. The only logistical issue is that it's hard to find a wordpress host for
free that allows you to use a custom domain (she's about to graduate from school so it's important
that she have a real website name). For that privilege wordpress.org charges you $8/mon, which,
while I may know nothing about Wordpress, I find to be crazy.&lt;/p&gt;
&lt;p&gt;So instead I went around searching for how to shoehorn Wordpress onto Heroku's Cedar stack (which is
where Django ends up). Luckily others have already had this desire and it was super easy!  Using
this &lt;a href="https://github.com/mhoofman/wordpress-heroku"&gt;github&lt;/a&gt; repository and README I had the
wordpress site up and running in seconds. Woot!&lt;/p&gt;&lt;/div&gt;</description><guid>http://www.npcompleteheart.com/posts/i-made-a-wordpress-site-on-heroku.html</guid><pubDate>Fri, 01 May 2015 14:41:25 GMT</pubDate></item><item><title>Things keep a-changing</title><link>http://www.npcompleteheart.com/posts/things-keep-a-changing.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;I guess my post on (Django and MongoDB in 2015)[www.npcompleteheart.com/posts/django-and-mongodb-in-2015] 
was prophetic, because I just changed my website over last week. It seemed that everytime I wanted to post 
a blog it was from an IPython notebook, which either meant reconfiguring the site to display IPython notebooks 
and get the css right or keep on with the semi-arduous process of converting the notebook to html (writing 
out &lt;code&gt;pre&lt;/code&gt; blocks and all that). At this point it made me feel like that barrier was keeping me from blogging. 
Since producing at least a blog per month is on my New Year's resolutions list I decided to change up the 
site to hopefully make it more conducive to me blogging.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;This time around, I really just wanted something simple that had IPython notebook support baked in
from the start. I wanted it to be lightweight, support markdown input (since it seemed like I'm the
last human alive writing in html and I could never remember the damn markdown symbols for that
reason), and I guess be a little bit anachronistic. For these reasons I just decided to go with
(Nikola)[www.getnikola.org]. It looked cool, sounded simple (loved the part in the tutorial that
just said, &lt;code&gt;don't read this tutorial, just start using&lt;/code&gt;), and it seems like the darkhorse in the
race against Pelican. In any case, that was enough to persuade me to start using it.&lt;/p&gt;
&lt;p&gt;The switch was pretty simple with the exception of the fact that I had to edit all my old posts to
make them appear like I wanted on this website. I would write more, but there really wasn't much
more to it than what's contained in teh basic introduction. The only thing that really held me up
was getting Git Pages to work. Most of the methods written on the internet weren't working for me
so I just did the lazy thing and made a separate repository for Github to serve as my website.&lt;/p&gt;
&lt;p&gt;Most things made it over okay, with the only real holdovers being the D3 visualizations with
javascript. I know that it's possible to move them over using an IPython notebook I just haven't had
the time yet. Through this process I also found out that someone limited the Folium mapping package
to only 6 colors again for apparently no fucking reason! Yay open source! Oh, and I killed some
high-traffic pages that I hated just because, like how to make a volcano plot. It was just too old
and out of date.&lt;/p&gt;
&lt;p&gt;Otherwise, the website isn't completely up yet (I need to add the projects back in, maybe give the
detail pages on individual publications) and figure out how to get some more javascript working (I know
completely against the whole starting premise). &lt;/p&gt;&lt;/div&gt;</description><guid>http://www.npcompleteheart.com/posts/things-keep-a-changing.html</guid><pubDate>Fri, 01 May 2015 13:50:41 GMT</pubDate></item><item><title>Computational research, no longer a red-headed stepchild!</title><link>http://www.npcompleteheart.com/posts/computational-research-no-longer-a-red-headed-stepchild.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;This week I had the, almost obscene, pleasure of participating in Northwestern’s Computational
Research Day as a chairperson and poster judge.I typically cringe at the thought of attending
conferences and symposia, since I am mainly a homebody (I love my desk, computer, research, and
daily schedule), but at the symposium on Tuesday I felt constantly excited and engaged. The variety
and quality of research presented was excellent and the diverse topics covered kept the gears
turning in my head.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;Most importantly, the sense of camaraderie that I felt amongst these computational researchers kept
me energized. I wish that this event existed when I started graduate school. As a computational
researcher that was purportedly a biologist, it was easy to feel that I was a part of a group that I
was a red-headed stepchild. My home program was primarily concerned with experimentation or the
application of rote computational biology (i.e. anything involving a computer that would have been
published since the early 90s). Most of the other departments that included some computational
research were ones where computational methods were de rigeur and field specific research dominated.
Instead I was looking more for what made me gravitate towards the Northwestern Institute for
complexity, computational researchers that were applying new techniques and conducting
interdisciplinary research.&lt;/p&gt;
&lt;p&gt;I can’t wait to see what new problems people will tackle as the quantity, diversity, and
interdisciplinarity of computational research continues to expand. Because of my experience in
biology, I realize how important it is to bring other fields and researcher into the fold of the
computational community. It will help change how people view their research problems and hopefully
lead to better research. This is what drives me to help put on the bootcamps as an introduction to
programming and data science. It’s also why I believe that events like the Computational Research
Day are integral to a healthy, productive research environment at a University.&lt;/p&gt;
&lt;p&gt;Keep Growing, Computational Research! Keep Growing!&lt;/p&gt;&lt;/div&gt;</description><guid>http://www.npcompleteheart.com/posts/computational-research-no-longer-a-red-headed-stepchild.html</guid><pubDate>Wed, 22 Apr 2015 00:56:52 GMT</pubDate></item><item><title>MongoDB is for researchers</title><link>http://www.npcompleteheart.com/posts/mongodb-is-for-researchers.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;Over the past three years I’ve been something of an evangelist for using MongoDB. This stance has
drawn derision from some outside the lab, which frequently forces me to clarify in what
circumstances I think MongoDB (or NoSQL in general) is so great. Unfortunately, I’ve been too lazy
to put those thoughts into writing, so this is my long overdue explanation and the first in a series
of posts describing how I use MongoDB daily.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;h3&gt;MongoDB is for &lt;strike&gt;lovers&lt;/strike&gt; researchers and scientists&lt;/h3&gt;
&lt;p&gt;So I think the first question to tackle is why use a database at all? Here are the three basic
reasons that caused me to make a switch.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Speed.&lt;/strong&gt; If you’ve ever explored the parameter landscape of a model then you’ve likely
experienced the point when typing &lt;code&gt;ls *&lt;/code&gt; in your results folder can bring your system to its knees.
While there are ways to work around this problem (creating subfolders, smart naming conventions to
get groups of files), you can also just switch to storing the results in a database. Databases are
designed for storing millions of records easily.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Queries.&lt;/strong&gt; How many runs with parameters (rho, mu, sigma) have finished? Okay, now how many
of those runs have a final value of y? Not nearly as easy to answer is it? At best it would require
looking at the final line of every file. At worst, with some odd encoding scheme or additional forms
of output in the same file this could require parsing every file. Databases make answering these
types of questions quick and easy.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Portability.&lt;/strong&gt; Your data files need to be in a specified directory for your code to read
it. A folder for all your results needs to be in the right place too. Any time you want to test your
code those parts all need to be there and hopefully the ‘there’ isn’t in the same folder as your
code (I am an unabashed proponent of separating code, data, and results). For me, switching between
a laptop, workstation, and two different clusters, this can lead to some annoying inconsistencies
with file availability (specifically with large data files). Storing your data in a database with a
static IP address makes it easy to access anywhere.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is the start to why I think it’s a good idea to use a database, with another lurking reason
being that working knowledge of databases is required outside academia. If none of those reasons
resonate with you to explore these options then don’t worry, not everyone has the same research
problems as me.&lt;/p&gt;
&lt;h3&gt;So why X technology over Y technology?&lt;/h3&gt;
&lt;p&gt;If you wade too far into the internet you’ll find out that MongoDB is a type of NoSQL database and
that other types of databases are SQL databases. If you wade even a little bit further then a
torrent of flame wars will come pouring out of your monitor and you should just shut your eyes,
cover your ears, and pull your computer’s power plug out. Hopefully the following reasons will make
it slightly clear what the differences are without having to go dive into the recesses of the
internet. &lt;/p&gt;
&lt;p&gt;Ease of use - i.e. Schemaless. What does schemaless really mean?
When you use a SQL database you need to first create a database and then a table. Then once you
create a table you must give commands or use a GUI to establish the number of columns, the names of
columns, and, most importantly, the data type that each column can hold. When a record is entered
into the database it must have all those fields. If you decide to change your code and need to store
additional data fields then you must alter the table first (or else suffer an error!).&lt;/p&gt;
&lt;p&gt;With MongoDB you create a database and then insert a document into it. It will even lazily create
the collection that you told it to use. It can have any number of fields (or keys in Mongo/document
speak) and each key can be named however you want. The twist is when you go to insert a second
document. The second document doesn’t need the same number of keys, or key names, or even the
datatypes of the values associated with each key name. It allows you to do whatever at any time,
with any document in the database.&lt;/p&gt;
&lt;p&gt;Now this freedom is considered to be a flaw in some minds, but all I see is that the onus of
consistency is on the programmer (i.e. you). In the context of a single person, a small group, or a
research lab I don’t think that it’s much to expect that everyone act responsibly and document what
they’re doing (either in the README for the project and/or with explicit key names). The most
important thing to remember is that just about any technology can be detrimental to the workings of
a project if in the hands of an irresponsible idiot.&lt;/p&gt;
&lt;h4&gt;Dictionaries!&lt;/h4&gt;
&lt;p&gt;So this stems from being a pythonista, but when  I code I store things as dictionaries or classes
typically. MongoDB lets me shove that directly into the database since it works with natively with
dictionaries (Mongo’s data store is a BSON, which is more or less a JSON, which is almost a
dictionary). This isn’t so when working with a SQL database, since each record is stored in a row
(think of a CSV file), and for me this is a huge differentiation and selling point. &lt;/p&gt;
&lt;p&gt;Complex data structures can be natively stored in MongoDB and they are directly returned when I
query them.  So for me, when I run a simulation and there is a class keeping track of the time
evolution of the system at the end of the run I can just calculate whatever additional metrics are
necessary and save the dictionary into the database. When I need to analyze the results I can either
roll directly with the dictionary and start analyzing, it’s pretty simple.&lt;/p&gt;
&lt;h4&gt;Complex values.&lt;/h4&gt;
&lt;p&gt;I have stored datetime as a value, which isn’t really that special. What is special is when I store
a networkx graph object. Mongo will let you shove a fair number of things into it without requiring
you to convert them to a string. This is not only handy, but it cuts out code and processing steps
on file loading.&lt;/p&gt;
&lt;h4&gt;MapReduce.&lt;/h4&gt;
&lt;p&gt;This is more of a footnote but MapReduce is a great feature and can turn 24 hours of computation
time into one fairly quickly. &lt;/p&gt;
&lt;p&gt;These are the basic reasons why I use MongoDB, both in comparison to a file system and a SQL
database. I will never say that it’s the fastest or the best solution from a technical standpoint,
but it is the quickest and easiest solution in regards to my time, which is the most important thing
in my mind. I’m a researcher, not someone setting up production databases or something soul crushing
like that ;)&lt;/p&gt;&lt;/div&gt;</description><guid>http://www.npcompleteheart.com/posts/mongodb-is-for-researchers.html</guid><pubDate>Tue, 31 Mar 2015 18:49:20 GMT</pubDate></item><item><title>Sometimes, on the first day of class, I walk around to find a room where the instructor is late</title><link>http://www.npcompleteheart.com/posts/sometimes-on-the-first-day-of-class-i-walk-around-to-find-a-room-where-the-instructor-is-late.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;p&gt;&lt;img alt="comic" src="http://www.npcompleteheart.com/images/first_day_comic.jpeg"&gt;&lt;/p&gt;</description><guid>http://www.npcompleteheart.com/posts/sometimes-on-the-first-day-of-class-i-walk-around-to-find-a-room-where-the-instructor-is-late.html</guid><pubDate>Tue, 03 Feb 2015 14:48:35 GMT</pubDate></item><item><title>How does cooperation evolve?</title><link>http://www.npcompleteheart.com/posts/how-does-cooperation-evolve.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;
I just finished reading Axelrod's &lt;i&gt;The Complexity of Cooperation&lt;/i&gt; and I have to say, it's one of the better scientific books that I can remember reading (period). This is surprising (for me), since it's really just a collection of seven of his published papers along with some commentary. However, his papers are so well written and the commentary is brilliant, especially for someone pursuing research as a career, since it not only provides insight into the genesis of the work but also how it was regarded by journals.
&lt;/p&gt;

&lt;p&gt;
This case is especially so for Chapters 4/5/7, which are concerned with his use of Landscape theory to predict alliances between nations and companies and the diffusion of culture. For all of these papers, he discusses the difficulties in publishing these pieces and then their lack of acceptance in the field at the time. For the landscape theories the lack of acceptance was primarily due to the nonexistence of rational agents and decision-making, while in the culture diffusion model it centered on the absence of "politics of any kind", as so succinctly stated by one reviewer.
&lt;/p&gt;

&lt;!-- TEASER_END --&gt;

&lt;p&gt;
In a hilarious twist, or maybe it's based solely on my anti-authoritarian leanings, these three papers were far and away my favorites of the entire compilation. The two landscape theory papers were concerned with predicting the formation of alliances, with Chapter 4 being focused on predicting the split of countries in World War II and Chapter 5 consortium alliances in setting Unix standards in the late 1980s. He describes the distaste that game theorists and (probably) economists had for this work since it didn't have a true "game" or rational decision agents, and instead was inspired by research on spin glass models and is, in reality, a method to find the energy landscape of possible group formations. In a somewhat hilarious turn, despite knowing more than a few people who have done research on spin glass models, this is probably the first piece of work that really made me appreciate the value that is in this model (which, honestly, was lacking in me before). 
&lt;/p&gt;

&lt;p&gt;
But the two run away chapters in my mind were 6 and 7. Chapter 6 was focused on the evolution of new political actors that are superstructures of smaller actors (think the formation of a nation from colonies a la the United States) which was investigated with what he called the tribute model. This was a relatively simple model where an actor in the game is selected at random and then is given the opportunity to attack another actor, which it will do so long as any other actor has less wealth than it does (otherwise it would be trounced in a fight). The opposing actor in this confrontation has the choice between fighting or paying tribute, and it simply selects whichever option costs less. Whenever an actor pays a tribute to another actor this builds a bond between the two and this bond comes into play when one of the actors is attacked by an outsider. What happens then is any actor in an alliance with the attacked country has this obligation to defend its ally, which further reinforces the bond between the two initial actors. Through simulation results results Axelrod showed that these simple behaviors were able to cause the formation of essentially new political actors, namely superstructures of actors with typically one dominant actor that was the caretaker of several minor actors. Furthermore, despite the simplicity of this model there is considerable complexity in the dynamics, namely that it there are many, distinctly different scenarios that can play out at random at 10,000 or more steps into the simulation.
&lt;/p&gt;

&lt;p&gt;
Chapter 7 was an, almost unbelievably simple model that focused on cultural diffusion. Each agent in a 2-dimensional grid was initialized with a vector of cultural traits for &lt;i&gt;n&lt;/i&gt; cultural features. The play of this model was very simple, at random an agent within the grid was selected and with some probability, based on homophily with a neighbor, a dissimilar trait is diffused. There were four major results from this model, with two being relatively intuitive and the other two being momentary head-scratchers.
&lt;/p&gt;

&lt;p&gt;
First, there is the concern of geography. A relatively intuitive result is that the number of stable groups in the population decreases as each agent has more neighbors. This is similar to the first question that comes to mind now, which is what about the internet. What it basically says is that as we have the ability to contact more people, it is more likely we will find a similar person to share traits with. This helps the system reach a relatively smaller number of overall groups. The other was with the size of the grid. In this case there are relatively few cultural groups when the grid is small (say a 5x5 grid) and more as the grid grows into moderate sizes (about 20x20), so far so good in terms of making sense. However, after this point the number of stable final groups starts to decrease, now why is that? Based on the simulations we find out that in a very large grid most of the time is actually just spent with two competing dialects (i.e. a majority with a dialect vector of  and a minority with ) fighting each other. However, while this process is like a random walk, there is a twist that there is a boundary, basically change in the size of the populations can only occur at the border were a majority agent and a minority agent meet. This means that it is most likely that the majority will subsume the minority, it is just that it will take a longer time than in a smaller grid. In the larger population grid, this border is larger which means that while it seems like there will be &lt;b&gt;more&lt;/b&gt; cultural regions to (which is true), there ends up being &lt;b&gt;less&lt;/b&gt; distinct cultural regions because of the overlap in beliefs and this establishment of cultural zones. Basically in any case where two agent share one feature there is the chance that they will assimilate.
&lt;/p&gt;

&lt;p&gt;
Second, is the concern of cultural traits and features, which is slightly more straightforward. The greater the number of cultural traits for a given feature, the more cultural regions that can be expected to form. This is fairly simple, as the number of opinions/options on a distinct issue grow, the easier it is to be surrounded by someone who does not share a similar viewpoint on any of the cultural features. This is something that made me immediately think back to high school, with music and the cliques that formed around them. As each musical genre continued to subdivide (I myself was in the skacore clique), the smaller and more numerous the groups became. What is mildly surprising is the result for cultural features, which shows that as the number of features &lt;b&gt;increases&lt;/b&gt; the number of groups &lt;b&gt;decreases&lt;/b&gt;. After a hot minute this makes sense though, the more issues that exits the easier it is to find &lt;b&gt;at least one&lt;/b&gt; that I agree on with a neighbor. After this initial icebreaker of an agreement it's much easier to open the lines of communication, so to speak, and begin transferring ideas and opinions.
&lt;/p&gt;

&lt;p&gt;
However, the main question that I still have of this model is one of cultural drift. Axelrod addressed this (and the reviewers requested) but the simulation results proved too thorny to unpack easily. I haven't had the time to do a literature search yet so this may be an answered question (the book is 17 years old), but it seems to be a fundamental one. This is especially so with my research interest on the diffusion of innovations within a system What is necessary for new "traits" that are introduced to survive, especially after a system has already reach a steady state? Is it necessary for a system to be in a dynamic state for these traits to survive? I think that these are interesting questions that may be out of reach for the model but that's never stopped me from wasting a week of work....
&lt;/p&gt;

&lt;p&gt;
In any case this is wonderful book that only takes about a night and a glass of whiskey to finish, which means that you really can't go wrong.
&lt;/p&gt;&lt;/div&gt;</description><guid>http://www.npcompleteheart.com/posts/how-does-cooperation-evolve.html</guid><pubDate>Sun, 25 Jan 2015 14:47:41 GMT</pubDate></item><item><title>Do yourself a favor...</title><link>http://www.npcompleteheart.com/posts/do-yourself-a-favor.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;iframe width="100%" height="450" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/172185143&amp;amp;auto_play=false&amp;amp;hide_related=false&amp;amp;show_comments=true&amp;amp;show_user=true&amp;amp;show_reposts=false&amp;amp;visual=true"&gt;&lt;/iframe&gt;

&lt;p&gt;
and listen to this wonderful collaboration. It's a new album coming out in February between Ghostface Killah (shame on you if you don't know) and BadBadNotGood (a jazz trio from Toronto). The beats and backing music on these tracks are just nothing but...sublime.
&lt;/p&gt;&lt;/div&gt;</description><guid>http://www.npcompleteheart.com/posts/do-yourself-a-favor.html</guid><pubDate>Sun, 25 Jan 2015 14:44:42 GMT</pubDate></item><item><title>Django and MongoDB in 2015</title><link>http://www.npcompleteheart.com/posts/django-and-mongodb-in-2015.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;
So believe it or not my post from 2013 on setting up a Django website with a MongoDB back-end is still one of my most visited pages. Actually that's not hard to believe, it's not like this site is a trove of content (but it's a goal for 2015 to get content more consistently up here!), but 2013 is more than ancient in technology times so I just wanted to do a quick revisit.
&lt;/p&gt;

&lt;h5&gt;Should I follow that blog post and set up a Django app with MongoDB?&lt;/h5&gt;

&lt;h5&gt;Hell no, definitely not now&lt;/h5&gt;

&lt;!-- TEASER_END --&gt;

&lt;p&gt;
Admittedly, my website still uses it but that is out of pure laziness and a lack of desire to migrate/work up something new. As is, I'm pretty sure that I have another year and a half before there's a catastrophic problem or a new feature that I absolutely have to have before I need to redo it all. And therein lies the biggest problem with trying to go down this path.
&lt;/p&gt;

&lt;p&gt;
The reason that I wanted to use MongoDB so much was its native usage of JSON (well, BSON) so that I could quickly and easily store data or javascript code for quick on-the-fly visualizations. I wanted Django for its robust admin backend (still why I love it! Nothing else that I've used has anything quite as good). However, now with the newest version of PostgreSQL and its support of JSON as a column type it's possible to get everything I want out of a website using a traditional SQL database (actually the one that Heroku has always wanted you to use anyways). Granted, this means going back to the world of migrations will be a pain but it's easy enough concession to be back on the main branch of Django development.
&lt;/p&gt;

&lt;h5&gt;But what if I really want to put the two together still?&lt;/h5&gt;

&lt;p&gt;
There could still be a pretty valid reason why you want an easy admin interface and a NoSQL backend, but in that case I don't think any of the old instructions may still apply with all of the time that has passed. In all honesty, if you're not making a blog site (or you won't be blogging consistently like me, womp womp) I would suggest not using Django at all and using a custom flask app (which would be simple enough) instead and provide lots of flexibility. If you were tr Buying to make a site as a product, then I wouldn't recommend python at all then and recommend node.js due to the performance benefits. 
&lt;/p&gt;

&lt;p&gt;
All in all, it's hard to say what to do in this new world where static site generators are the new hot thing. I still like having the ability to write and save drafts on my website instead of being stuck to a checked out instance of my website (I switch computers and locations...too often still). But in any case, I still like using Heroku even if it is apart of the giant SalesForce conglomerate.
&lt;/p&gt;&lt;/div&gt;</description><guid>http://www.npcompleteheart.com/posts/django-and-mongodb-in-2015.html</guid><pubDate>Fri, 02 Jan 2015 14:46:17 GMT</pubDate></item><item><title>2014 Literature Round up</title><link>http://www.npcompleteheart.com/posts/2014-literature-round-up.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;img alt="book review" src="http://www.npcompleteheart.com/images/2014_roundup_zpsff71df98.png"&gt;&lt;/p&gt;
&lt;p&gt;
Twenty fourteen, what can I say. It was consistent and not always pretty but I made it through. I made my first actual contribution to an &lt;a href="http://github.com/python-visualization/folium/commits?author=adamrpah"&gt;open source project&lt;/a&gt;, learned the value of gists, started opening up &lt;a href="http://github.com/adamrpah/GALE"&gt;GALE&lt;/a&gt; to the world, and successfully made it through putting on a one week bootcamp transform non-coders into pythonistas. I at least got one manuscript out (albeit a &lt;a href="http://npcompleteheart.com/article/big-data-what-is-it-and-what-does-it-mean-for-card/"&gt;review&lt;/a&gt;) and a conference presentation at &lt;a href="http://npcompleteheart.com/article/unzipping-zip-codes-a-methodology-to-assign-de-ide/"&gt;AMIA&lt;/a&gt;. I even launched Socraticc as a part of OmegaK and brought it right down after no one wanted to use it after a few months.
&lt;/p&gt;

&lt;!-- TEASER_END --&gt;

&lt;p&gt;
But what I'm really happy about is that I've mostly maintained a promise to myself that I would be consistently reading throughout the year. This is my quick year in review of what I've made it through, if for nothing else other than posterity.
&lt;/p&gt;

&lt;p&gt;&lt;br&gt;
&lt;/p&gt;&lt;h5&gt;Hard-Boiled Wonderland and the End of the World&lt;/h5&gt;
&lt;h5&gt;1Q84&lt;/h5&gt;
&lt;h5&gt;Colorless Tsukuru Tazaki and His Years of Pilgrimage&lt;/h5&gt;
&lt;h5&gt;Norwegian Wood&lt;/h5&gt;
&lt;h5&gt;Kafka on the Shore&lt;/h5&gt;
&lt;h5&gt;The Wind-Up Bird Chronicle&lt;/h5&gt;
&lt;h5&gt;—Haruki Murakami&lt;/h5&gt;
&lt;p&gt;
This is the year wherein I found Haruki Murakami and was simply blown away. The amount of emotion and alienation induced by his writing is amazing and for most of the books I've tried to read them with a soundtrack that corresponds to the book (from whichever classical artists figured most prominently in the work) and it's been a wonderful experience. I can't recommend these works enough. I just finished &lt;i&gt;Colorless Tsukuru Tazaki and His Years of Pilgrimage&lt;/i&gt; and, while I didn't feel as strongly about it while reading it, I'm already starting to form a much stronger attachment and opinion about it. You can't go wrong with any of these books, but my favorite would have to be &lt;i&gt;Kafka on the Shore&lt;/i&gt;.
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;/p&gt;&lt;h5&gt;Count Zero&lt;/h5&gt;
&lt;h5&gt;Mona Lisa Overdrive&lt;/h5&gt;
&lt;h5&gt;—William Gibson&lt;/h5&gt;
&lt;p&gt;
I finally finished the Neuromancer trilogy after seeing William Gibson do a Q&amp;amp;A during the Chicago Humanities Festival (his answers were wonderful and insightful, the questions were not so great however). I realized that I hadn't finished the rest of the trilogy so I went back and finished it (I was switching back and forth between Gibson and Stephenson when I read &lt;i&gt;Neuromancer&lt;/i&gt;). These books were actually great additions to the series and I felt that they really enhanced the story in &lt;i&gt;Neuromancer&lt;/i&gt;. Next year, I'll start &lt;i&gt;The Peripheral&lt;/i&gt; once I'm certain I've forgotten all the spoilers I've seen. 
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;/p&gt;&lt;h5&gt;Statistics&lt;/h5&gt;
&lt;h5&gt;—David Freedman&lt;/h5&gt;
&lt;p&gt;
I never took an introductory statistics course, and as such my understanding of the statistician "language" and viewpoint was pretty lacking (I'm more a fan of Monte Carlo methods and bootstrapping to test statistical hypotheses), so I wanted to shore up my ability to understand that viewpoint and the language. This book was very well written and I really enjoyed all of the examples given. Now I just need to finish Andrew Gelman's &lt;i&gt;Bayesian Data Analysis&lt;/i&gt; and Tibshirani's &lt;i&gt;Elements of Statistical Learning&lt;/i&gt; next year.
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;/p&gt;&lt;h5&gt;The Informers&lt;/h5&gt;
&lt;h5&gt;American Psycho&lt;/h5&gt;
&lt;h5&gt;—Bret Easton Ellis&lt;/h5&gt;
&lt;p&gt;
I really like some of Bret Easotn Ellis' work, and &lt;i&gt;American Psycho&lt;/i&gt; was a great read but &lt;i&gt;The Informers&lt;/i&gt; was completely forgettable. 
&lt;/p&gt;
&lt;p&gt;&lt;br&gt;
&lt;/p&gt;&lt;h5&gt;Do androids dream of electric sheep?&lt;/h5&gt;
&lt;h5&gt;We can remember it for you wholesale&lt;/h5&gt;
&lt;h5&gt;Philip K. Dick&lt;/h5&gt;
&lt;p&gt;
Somehow these books slipped through the cracks when I read the rest of Dick's work earlier. &lt;i&gt;Do androids dream of electric sheep?&lt;/i&gt; is a wonderful story, that I was surprised to read in terms of how much it differed from Ridley Scott's movie adaptation. The same can be said for &lt;i&gt;We can remember it for you wholesale&lt;/i&gt;, the funny thing is that I'm rather surprised with how successful the adaptations of Dick's work are (even when they deviate from the written story) while Gibson made an explicit comment about how he hated the adaptation of Johnny Mnemonic (which I can understand after seeing it).
&lt;/p&gt;
&lt;p&gt;
And that's the end of 2014, time to make some goals for 2015 and deliver on them.
&lt;/p&gt;&lt;/div&gt;</description><guid>http://www.npcompleteheart.com/posts/2014-literature-round-up.html</guid><pubDate>Wed, 31 Dec 2014 14:43:24 GMT</pubDate></item><item><title>A story of human reasoning</title><link>http://www.npcompleteheart.com/posts/a-story-of-human-reasoning.html</link><dc:creator>Adam Pah</dc:creator><description>&lt;div&gt;&lt;h5&gt;(a first part summary of essays contained within Gigerenzer and Selten's [eds.] &lt;i&gt;Bounded Rationality: The Adaptive Toolbox&lt;/i&gt;)&lt;/h5&gt;

&lt;p&gt;
What will I make for dinner? A relatively simple question that plagues me most nights of the week. However, what is a relatively simple task, albeit one that can be arduous for me, unpacks a can of worms in basic research: through what &lt;b&gt;mental process&lt;/b&gt; do I decide what to make?
&lt;/p&gt;

&lt;p&gt;
Admittedly this is a silly question, but other questions used to motivate this issue are roughly as silly. The point is merely to illuminate the difference between decision-making paradigms. Classically, or as far as my knowledge extends to, we would typically think of solving this problem through maximizing our utility, searching for the best possible answer (the global optima). In the context of our dinner problem, the steps to solving our problem would look like:
&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;mentally assemble all of the possible recipes&lt;/li&gt;
&lt;li&gt;determine which recipes had all of the possible recipes&lt;/li&gt;
&lt;li&gt;determine which recipes had all available ingredients&lt;/li&gt;
&lt;li&gt;account for the spoilage dates of all the ingredients&lt;/li&gt;
&lt;li&gt;identify the meal that would provide the maximum utility (one that uses the most ingredients that are about to spoil the soonest while being feasible to make)&lt;/li&gt; 
&lt;/ol&gt;


&lt;!-- TEASER_END --&gt;

&lt;p&gt;
The reality that this decision methodology ignores are the constraints in the real world. When I start to consider what to cook, the entire process has typically started because I'm hungry. This imposes a very real time constraint because if too much time passes I become what is known as "hangry" (both "hungry" and "angry" at the same time). In the process of maximizing my utility I generally ignore this time constraint and instead suffer in the search for the optimal answer. The situation considerably worsens for me if I am a poor cook and have few or no recipes memorized, since this introduces researching online first to identify possible recipes. There is also the additional kink of, how do I assess when I have spent enough time finding recipes so that the optimal recipe would be in the list of recipes I now possess? I could continue to look further and make comparisons, but this could rapidly lead to a situation where I am continually conducting research to determine if my prior research is sufficient. This leads to decision paralysis and the possibility of infinite regress, i.e. where I just keep going further down the rabbit hole of preparing to make my decision without making any progress towards resolving it.
&lt;/p&gt;

&lt;p&gt;
Crap. At this rate I might not start eating dinner until tomorrow or next week!
&lt;/p&gt;

&lt;p&gt;&amp;lt;&amp;gt;
&lt;/p&gt;&lt;p&gt;
This doesn't really seem to reflect my typical reality though. There is a fairly serious time crunch, wherein I must decide before hanger sets in. 
&lt;/p&gt;
&lt;p&gt;
In the early 1950s, Herbert Simon set out an alternative theory of &lt;i&gt;satisficing&lt;/i&gt; (stemming form the concepts of satisfy and suffice). Using this heuristic we actually go about decision-making in a different manner. Within this paradigm I instead set some minimum value of acceptability and instead evaluate each possible dish as I think of it. As I evaluate some dish I calculate its score based on the number of ingredients it uses that are about to spoil. As soon as I think of a dish that has a score greater than my set point I start cooking. I don't worry if I have the most optimal dish, I instead just start cooking. 
&lt;/p&gt;

&lt;p&gt;
This is where the bounds on rationality come from, I use a heuristic to achieve a locally optimal solution since it is more than likely good enough and fulfills normal criteria (like a lack of unlimited time). A fully rational agent (in the economic sense) would seek the optimal choice in the situation. An irrational agent would choose at random. A last class, which some like to equate to bounded rationality but differs, is a ration agent under constraints. We could say that in our dinner problem I am aware of the available time constraint and use that as a factor in my utility. However, in this situation I am still seeking the global maximum utility within the constraints of my problem, which differs from my heuristic of take the first answer that satisfices my concerns.
&lt;/p&gt;

&lt;p&gt;
And at this point I'll conclude my first summary of the works. From here we start exploring the role that the environment plays in decision-making, and our ability to exploit it when it doesn't change rapidly with heuristics, but that seems like a good second topic.
&lt;/p&gt;&lt;/div&gt;</description><guid>http://www.npcompleteheart.com/posts/a-story-of-human-reasoning.html</guid><pubDate>Mon, 24 Nov 2014 14:41:27 GMT</pubDate></item></channel></rss>